#!/usr/bin/env python
"""
Pipiads RPAç³»ç»Ÿä¸»å¯åŠ¨è„šæœ¬
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œå®ç°å®Œæ•´çš„è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹
"""

import schedule
import time
import logging
import argparse
import sys
import signal
import os
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import json

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_collector import PipiadsCollector
from data_processor import DataProcessor
from report_generator import ReportGenerator
from human_collaboration import HumanCollaborationManager
from config import *

class PipiadsRPASystem:
    """Pipiads RPAç³»ç»Ÿä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.running = False
        self.components = {}
        self.session_stats = {
            'start_time': None,
            'tasks_completed': 0,
            'errors_encountered': 0,
            'last_successful_run': None
        }
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®ä¸»ç³»ç»Ÿæ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('PipiadsRPASystem')
        logger.setLevel(logging.INFO)
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(get_output_path(PATHS['activity_log'])), exist_ok=True)
        
        # æ–‡ä»¶å¤„ç†å™¨
        log_file = get_output_path(PATHS['activity_log'])
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # æ ¼å¼å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def _initialize_components(self):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        try:
            self.logger.info("åˆå§‹åŒ–RPAç³»ç»Ÿç»„ä»¶...")
            
            # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
            self.components['collector'] = PipiadsCollector()
            self.components['processor'] = DataProcessor()
            self.components['reporter'] = ReportGenerator()
            self.components['collaboration'] = HumanCollaborationManager()
            
            # æ³¨å†Œäººæœºåä½œå›è°ƒ
            self._register_collaboration_callbacks()
            
            self.logger.info("ç³»ç»Ÿç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _register_collaboration_callbacks(self):
        """æ³¨å†Œäººæœºåä½œå›è°ƒå‡½æ•°"""
        def handle_high_potential_product(item, result, comments):
            """å¤„ç†é«˜æ½œåŠ›äº§å“å®¡æ ¸ç»“æœ"""
            if result == 'approved':
                self.logger.info(f"Açº§äº§å“å·²æ‰¹å‡†å¼€å‘: {item['item_data'].get('product_name')}")
                # è¿™é‡Œå¯ä»¥è§¦å‘å¼€å‘æµç¨‹
            elif result == 'rejected':
                self.logger.info(f"Açº§äº§å“å·²æ‹’ç»: {comments}")
        
        def handle_data_anomaly(item, result, comments):
            """å¤„ç†æ•°æ®å¼‚å¸¸å®¡æ ¸ç»“æœ"""
            if result == 'accepted':
                self.logger.info("æ•°æ®å¼‚å¸¸å·²æ¥å—ï¼Œç»§ç»­å¤„ç†")
            else:
                self.logger.warning(f"æ•°æ®å¼‚å¸¸éœ€è¦å…³æ³¨: {comments}")
        
        # æ³¨å†Œå›è°ƒå‡½æ•°
        collaboration = self.components['collaboration']
        collaboration.register_callback('high_potential_product', handle_high_potential_product)
        collaboration.register_callback('data_anomaly', handle_data_anomaly)
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        self.logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
        self.running = False
    
    def daily_workflow(self):
        """æ¯æ—¥å®Œæ•´å·¥ä½œæµç¨‹"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ¯æ—¥å·¥ä½œæµç¨‹")
            workflow_start = datetime.now()
            
            # æ­¥éª¤1: æ•°æ®é‡‡é›†
            self.logger.info("ğŸ“Š æ­¥éª¤1: å¼€å§‹æ•°æ®é‡‡é›†")
            collected_data = self._execute_data_collection()
            
            if not collected_data:
                self.logger.error("æ•°æ®é‡‡é›†å¤±è´¥ï¼Œç»ˆæ­¢å·¥ä½œæµç¨‹")
                return False
            
            # æ­¥éª¤2: æ•°æ®å¤„ç†å’Œåˆ†æ
            self.logger.info("ğŸ”¬ æ­¥éª¤2: å¼€å§‹æ•°æ®å¤„ç†å’Œåˆ†æ")
            analysis_results = self._execute_data_processing(collected_data)
            
            if not analysis_results:
                self.logger.error("æ•°æ®å¤„ç†å¤±è´¥ï¼Œç»ˆæ­¢å·¥ä½œæµç¨‹")
                return False
            
            # æ­¥éª¤3: ç”ŸæˆæŠ¥å‘Š
            self.logger.info("ğŸ“ˆ æ­¥éª¤3: å¼€å§‹ç”ŸæˆæŠ¥å‘Š")
            report_files = self._execute_report_generation(analysis_results)
            
            # æ­¥éª¤4: äººæœºåä½œå¤„ç†
            self.logger.info("ğŸ¤ æ­¥éª¤4: å¤„ç†äººæœºåä½œä»»åŠ¡")
            self._handle_collaboration_tasks()
            
            # è®°å½•æˆåŠŸå®Œæˆ
            workflow_duration = datetime.now() - workflow_start
            self.session_stats['tasks_completed'] += 1
            self.session_stats['last_successful_run'] = datetime.now().isoformat()
            
            self.logger.info(f"âœ… æ¯æ—¥å·¥ä½œæµç¨‹å®Œæˆï¼Œè€—æ—¶: {workflow_duration}")
            
            # å‘é€å®Œæˆé€šçŸ¥
            self._send_completion_notification(analysis_results, report_files, workflow_duration)
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ¯æ—¥å·¥ä½œæµç¨‹å¤±è´¥: {e}")
            self.session_stats['errors_encountered'] += 1
            self._send_error_notification(str(e))
            return False
    
    def _execute_data_collection(self) -> Optional[str]:
        """æ‰§è¡Œæ•°æ®é‡‡é›†"""
        try:
            collector = self.components['collector']
            
            # å¯åŠ¨é‡‡é›†ä¼šè¯
            if not collector.start_session():
                raise Exception("é‡‡é›†ä¼šè¯å¯åŠ¨å¤±è´¥")
            
            try:
                # ç™»å½•
                if not collector.login():
                    raise Exception("ç™»å½•å¤±è´¥")
                
                # è®¾ç½®ç­›é€‰å™¨
                if not collector.setup_search_filters():
                    raise Exception("ç­›é€‰å™¨è®¾ç½®å¤±è´¥")
                
                # è·å–ä»Šæ—¥å…³é”®è¯å¹¶æœç´¢
                today_keywords = get_today_keywords()
                self.logger.info(f"ä»Šæ—¥æœç´¢å…³é”®è¯: {today_keywords}")
                
                if not collector.search_products(today_keywords):
                    raise Exception("äº§å“æœç´¢å¤±è´¥")
                
                # ç›‘æ§ç«å“ï¼ˆå¦‚æœé…ç½®äº†ç«å“åˆ—è¡¨ï¼‰
                competitor_products = self._get_competitor_products()
                if competitor_products:
                    collector.monitor_competitors(competitor_products)
                
                # ä¿å­˜æ•°æ®
                if not collector.save_data():
                    raise Exception("æ•°æ®ä¿å­˜å¤±è´¥")
                
                # ç”Ÿæˆä¼šè¯æ€»ç»“
                session_summary = collector.generate_session_summary()
                self.logger.info(f"é‡‡é›†æ€»ç»“: {session_summary}")
                
                return get_output_path(PATHS['daily_scan_file'])
                
            finally:
                collector.close_session()
                
        except Exception as e:
            self.logger.error(f"æ•°æ®é‡‡é›†æ‰§è¡Œå¤±è´¥: {e}")
            return None
    
    def _execute_data_processing(self, data_file: str) -> Optional[Dict[str, Any]]:
        """æ‰§è¡Œæ•°æ®å¤„ç†å’Œåˆ†æ"""
        try:
            processor = self.components['processor']
            
            # å¤„ç†æ•°æ®
            analysis_results = processor.process_data(data_file)
            
            if not analysis_results:
                raise Exception("æ•°æ®å¤„ç†è¿”å›ç©ºç»“æœ")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦äººå·¥å®¡æ ¸çš„é¡¹ç›®
            self._check_for_human_review_items(processor)
            
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"æ•°æ®å¤„ç†æ‰§è¡Œå¤±è´¥: {e}")
            return None
    
    def _execute_report_generation(self, analysis_results: Dict[str, Any]) -> Dict[str, str]:
        """æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆ"""
        try:
            reporter = self.components['reporter']
            processor = self.components['processor']
            
            # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
            report_files = reporter.generate_full_report(
                processor.processed_data,
                analysis_results,
                processor.alerts
            )
            
            if report_files:
                self.logger.info(f"æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {list(report_files.keys())}")
            
            return report_files
            
        except Exception as e:
            self.logger.error(f"æŠ¥å‘Šç”Ÿæˆæ‰§è¡Œå¤±è´¥: {e}")
            return {}
    
    def _handle_collaboration_tasks(self):
        """å¤„ç†äººæœºåä½œä»»åŠ¡"""
        try:
            collaboration = self.components['collaboration']
            
            # è¿è¡Œç»´æŠ¤ä»»åŠ¡
            collaboration.run_maintenance_tasks()
            
            # æ£€æŸ¥é€¾æœŸé¡¹ç›®
            overdue_items = collaboration.check_overdue_items()
            if overdue_items:
                self.logger.warning(f"å‘ç° {len(overdue_items)} ä¸ªé€¾æœŸå®¡æ ¸é¡¹ç›®")
            
            # ç”Ÿæˆå®¡æ ¸ä»ªè¡¨æ¿
            dashboard_file = collaboration.generate_review_dashboard()
            if dashboard_file:
                self.logger.info(f"å®¡æ ¸ä»ªè¡¨æ¿å·²æ›´æ–°: {dashboard_file}")
                
        except Exception as e:
            self.logger.error(f"äººæœºåä½œä»»åŠ¡å¤„ç†å¤±è´¥: {e}")
    
    def _check_for_human_review_items(self, processor: DataProcessor):
        """æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦äººå·¥å®¡æ ¸çš„é¡¹ç›®"""
        try:
            if not processor.processed_data is None and len(processor.processed_data) > 0:
                collaboration = self.components['collaboration']
                
                # æ£€æŸ¥Açº§äº§å“
                a_level_products = processor.processed_data[
                    processor.processed_data.get('recommendation_level', '') == 'A'
                ]
                
                for _, product in a_level_products.iterrows():
                    collaboration.add_review_item(
                        item_type='high_potential_product',
                        item_data=product.to_dict(),
                        reason=f'Açº§é«˜æ½œåŠ›äº§å“éœ€è¦å¼€å‘å¯è¡Œæ€§è¯„ä¼°',
                        priority=Priority.HIGH,
                        due_hours=8
                    )
                
                # æ£€æŸ¥æ•°æ®å¼‚å¸¸
                if processor.alerts:
                    for alert in processor.alerts:
                        if alert.get('level') == 'warning':
                            collaboration.add_review_item(
                                item_type='data_anomaly',
                                item_data=alert,
                                reason=f'æ•°æ®å¼‚å¸¸éœ€è¦äººå·¥ç¡®è®¤: {alert.get("message")}',
                                priority=Priority.MEDIUM,
                                due_hours=24
                            )
                            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥äººå·¥å®¡æ ¸é¡¹ç›®å¤±è´¥: {e}")
    
    def _get_competitor_products(self) -> List[str]:
        """è·å–ç«å“äº§å“åˆ—è¡¨"""
        # è¿™é‡Œå¯ä»¥ä»é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“è¯»å–ç«å“åˆ—è¡¨
        return [
            "LEDé¢è†œä»ª",
            "ç»´Cç²¾åæ¶²",
            "ç»å°¿é…¸é¢è†œ",
            "èƒ¶åŸè›‹ç™½ç²‰"
        ]
    
    def competitor_monitoring(self):
        """ç«å“ä¸“é¡¹ç›‘æ§"""
        try:
            self.logger.info("ğŸ” å¼€å§‹ç«å“ä¸“é¡¹ç›‘æ§")
            
            collector = self.components['collector']
            
            if not collector.start_session():
                raise Exception("é‡‡é›†ä¼šè¯å¯åŠ¨å¤±è´¥")
            
            try:
                if not collector.login():
                    raise Exception("ç™»å½•å¤±è´¥")
                
                competitor_products = self._get_competitor_products()
                competitor_data = collector.monitor_competitors(competitor_products)
                
                if competitor_data:
                    # ä¿å­˜ç«å“æ•°æ®
                    competitor_file = get_output_path(PATHS['competitor_monitor_file'])
                    competitor_df = pd.DataFrame(competitor_data).T
                    competitor_df.to_csv(competitor_file, index=False, encoding='utf-8')
                    
                    self.logger.info(f"ç«å“ç›‘æ§å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜: {competitor_file}")
                
            finally:
                collector.close_session()
                
        except Exception as e:
            self.logger.error(f"ç«å“ç›‘æ§å¤±è´¥: {e}")
    
    def system_maintenance(self):
        """ç³»ç»Ÿç»´æŠ¤ä»»åŠ¡"""
        try:
            self.logger.info("ğŸ› ï¸ å¼€å§‹ç³»ç»Ÿç»´æŠ¤")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files()
            
            # å‹ç¼©æ—§æ—¥å¿—
            self._compress_old_logs()
            
            # ä¼˜åŒ–æ•°æ®åº“
            self._optimize_database()
            
            # æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€
            health_status = self._check_system_health()
            
            # ç”Ÿæˆç»´æŠ¤æŠ¥å‘Š
            self._generate_maintenance_report(health_status)
            
            self.logger.info("ç³»ç»Ÿç»´æŠ¤å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿç»´æŠ¤å¤±è´¥: {e}")
    
    def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        import glob
        
        temp_patterns = [
            'downloads/*.tmp',
            'outputs/*.tmp',
            '*.pyc',
            '__pycache__/*'
        ]
        
        for pattern in temp_patterns:
            files = glob.glob(pattern)
            for file in files:
                try:
                    os.remove(file)
                except:
                    pass
    
    def _compress_old_logs(self):
        """å‹ç¼©æ—§æ—¥å¿—æ–‡ä»¶"""
        import glob
        import gzip
        
        # å‹ç¼©7å¤©å‰çš„æ—¥å¿—
        cutoff_date = datetime.now() - timedelta(days=7)
        log_files = glob.glob('logs/*.log')
        
        for log_file in log_files:
            if os.path.getmtime(log_file) < cutoff_date.timestamp():
                try:
                    with open(log_file, 'rb') as f_in:
                        with gzip.open(f"{log_file}.gz", 'wb') as f_out:
                            f_out.writelines(f_in)
                    os.remove(log_file)
                except:
                    pass
    
    def _optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“"""
        try:
            collaboration = self.components['collaboration']
            
            import sqlite3
            conn = sqlite3.connect(collaboration.db_path)
            conn.execute('VACUUM')
            conn.close()
            
        except Exception as e:
            self.logger.warning(f"æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")
    
    def _check_system_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        try:
            import psutil
            
            health_status = {
                'timestamp': datetime.now().isoformat(),
                'cpu_usage': psutil.cpu_percent(interval=1),
                'memory_usage': psutil.virtual_memory().percent,
                'disk_usage': psutil.disk_usage('.').percent,
                'system_uptime': time.time() - psutil.boot_time(),
                'rpa_session_stats': self.session_stats
            }
            
            return health_status
            
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {}
    
    def _generate_maintenance_report(self, health_status: Dict[str, Any]):
        """ç”Ÿæˆç»´æŠ¤æŠ¥å‘Š"""
        try:
            report_content = f"""# ç³»ç»Ÿç»´æŠ¤æŠ¥å‘Š

**ç»´æŠ¤æ—¶é—´ï¼š** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## ç³»ç»ŸçŠ¶æ€
- **CPUä½¿ç”¨ç‡ï¼š** {health_status.get('cpu_usage', 0):.1f}%
- **å†…å­˜ä½¿ç”¨ç‡ï¼š** {health_status.get('memory_usage', 0):.1f}%
- **ç£ç›˜ä½¿ç”¨ç‡ï¼š** {health_status.get('disk_usage', 0):.1f}%

## RPAä¼šè¯ç»Ÿè®¡
- **å¯åŠ¨æ—¶é—´ï¼š** {self.session_stats.get('start_time', 'N/A')}
- **å®Œæˆä»»åŠ¡æ•°ï¼š** {self.session_stats.get('tasks_completed', 0)}
- **é‡åˆ°é”™è¯¯æ•°ï¼š** {self.session_stats.get('errors_encountered', 0)}
- **æœ€åæˆåŠŸè¿è¡Œï¼š** {self.session_stats.get('last_successful_run', 'N/A')}

## ç»´æŠ¤æ“ä½œ
- âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†
- âœ… æ—¥å¿—æ–‡ä»¶å‹ç¼©
- âœ… æ•°æ®åº“ä¼˜åŒ–
- âœ… ç³»ç»Ÿå¥åº·æ£€æŸ¥

---
*è‡ªåŠ¨ç”Ÿæˆçš„ç»´æŠ¤æŠ¥å‘Š*
"""
            
            report_file = get_output_path('./outputs/maintenance_report_{date}.md')
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            self.logger.info(f"ç»´æŠ¤æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            
        except Exception as e:
            self.logger.error(f"ç»´æŠ¤æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    def _send_completion_notification(self, analysis_results: Dict[str, Any], 
                                    report_files: Dict[str, str], 
                                    duration: timedelta):
        """å‘é€å®Œæˆé€šçŸ¥"""
        try:
            notification = {
                'type': 'workflow_completed',
                'timestamp': datetime.now().isoformat(),
                'duration': str(duration),
                'summary': {
                    'total_products': analysis_results.get('total_products', 0),
                    'high_potential_products': analysis_results.get('high_potential_count', 0),
                    'reports_generated': len(report_files)
                },
                'files': report_files
            }
            
            if NOTIFICATION_CONFIG['console_log_enabled']:
                print(f"âœ… å·¥ä½œæµç¨‹å®Œæˆé€šçŸ¥:")
                print(f"   å¤„ç†äº§å“: {notification['summary']['total_products']} ä¸ª")
                print(f"   é«˜æ½œåŠ›äº§å“: {notification['summary']['high_potential_products']} ä¸ª")
                print(f"   è€—æ—¶: {duration}")
                
        except Exception as e:
            self.logger.error(f"å‘é€å®Œæˆé€šçŸ¥å¤±è´¥: {e}")
    
    def _send_error_notification(self, error_message: str):
        """å‘é€é”™è¯¯é€šçŸ¥"""
        try:
            if NOTIFICATION_CONFIG['console_log_enabled']:
                print(f"âŒ å·¥ä½œæµç¨‹é”™è¯¯é€šçŸ¥: {error_message}")
                
        except Exception as e:
            self.logger.error(f"å‘é€é”™è¯¯é€šçŸ¥å¤±è´¥: {e}")
    
    def start_scheduler(self):
        """å¯åŠ¨ä»»åŠ¡è°ƒåº¦å™¨"""
        try:
            self.logger.info("ğŸ• å¯åŠ¨ä»»åŠ¡è°ƒåº¦å™¨")
            self.session_stats['start_time'] = datetime.now().isoformat()
            self.running = True
            
            # æ¸…é™¤æ‰€æœ‰ç°æœ‰ä»»åŠ¡
            schedule.clear()
            
            # å®‰æ’æ¯æ—¥ä»»åŠ¡
            schedule.every().day.at(SCHEDULE_CONFIG['daily_scan_time']).do(self.daily_workflow)
            schedule.every().day.at(SCHEDULE_CONFIG['competitor_monitor_time']).do(self.competitor_monitoring)
            
            # å®‰æ’ç»´æŠ¤ä»»åŠ¡
            schedule.every().day.at("02:00").do(self.system_maintenance)
            schedule.every().hour.do(lambda: self.components['collaboration'].check_overdue_items())
            
            self.logger.info("ğŸ“… å·²å®‰æ’çš„ä»»åŠ¡:")
            self.logger.info(f"   - æ¯æ—¥æ‰«æ: {SCHEDULE_CONFIG['daily_scan_time']}")
            self.logger.info(f"   - ç«å“ç›‘æ§: {SCHEDULE_CONFIG['competitor_monitor_time']}")
            self.logger.info(f"   - ç³»ç»Ÿç»´æŠ¤: 02:00")
            self.logger.info(f"   - å®¡æ ¸æ£€æŸ¥: æ¯å°æ—¶")
            
            # ä¸»å¾ªç¯
            while self.running:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
        except KeyboardInterrupt:
            self.logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
        except Exception as e:
            self.logger.error(f"è°ƒåº¦å™¨è¿è¡Œå¤±è´¥: {e}")
        finally:
            self.shutdown()
    
    def run_once(self, task_name: str = 'daily'):
        """è¿è¡Œå•æ¬¡ä»»åŠ¡"""
        try:
            self.logger.info(f"ğŸ¯ è¿è¡Œå•æ¬¡ä»»åŠ¡: {task_name}")
            self.session_stats['start_time'] = datetime.now().isoformat()
            
            if task_name == 'daily':
                return self.daily_workflow()
            elif task_name == 'competitor':
                self.competitor_monitoring()
                return True
            elif task_name == 'maintenance':
                self.system_maintenance()
                return True
            else:
                self.logger.error(f"æœªçŸ¥ä»»åŠ¡: {task_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"å•æ¬¡ä»»åŠ¡è¿è¡Œå¤±è´¥: {e}")
            return False
    
    def shutdown(self):
        """å…³é—­ç³»ç»Ÿ"""
        self.logger.info("ğŸ›‘ æ­£åœ¨å…³é—­RPAç³»ç»Ÿ...")
        self.running = False
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_stats = self.session_stats.copy()
        final_stats['end_time'] = datetime.now().isoformat()
        
        self.logger.info(f"ğŸ“Š ä¼šè¯ç»Ÿè®¡: {final_stats}")
        self.logger.info("RPAç³»ç»Ÿå·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Pipiads RPAç³»ç»Ÿ')
    parser.add_argument('--mode', choices=['scheduler', 'once'], default='scheduler',
                       help='è¿è¡Œæ¨¡å¼: scheduler(è°ƒåº¦æ¨¡å¼) æˆ– once(å•æ¬¡è¿è¡Œ)')
    parser.add_argument('--task', choices=['daily', 'competitor', 'maintenance'], 
                       default='daily', help='å•æ¬¡è¿è¡Œçš„ä»»åŠ¡ç±»å‹')
    parser.add_argument('--config-check', action='store_true', 
                       help='ä»…æ£€æŸ¥é…ç½®å¹¶é€€å‡º')
    
    args = parser.parse_args()
    
    # é…ç½®æ£€æŸ¥
    if args.config_check:
        try:
            validate_config()
            print("âœ… é…ç½®æ£€æŸ¥é€šè¿‡")
            return 0
        except Exception as e:
            print(f"âŒ é…ç½®æ£€æŸ¥å¤±è´¥: {e}")
            return 1
    
    # åˆ›å»ºRPAç³»ç»Ÿå®ä¾‹
    rpa_system = PipiadsRPASystem()
    
    try:
        if args.mode == 'scheduler':
            # è°ƒåº¦æ¨¡å¼ - æŒç»­è¿è¡Œ
            rpa_system.start_scheduler()
        else:
            # å•æ¬¡è¿è¡Œæ¨¡å¼
            success = rpa_system.run_once(args.task)
            return 0 if success else 1
            
    except Exception as e:
        rpa_system.logger.error(f"ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
        return 1
    finally:
        rpa_system.shutdown()

if __name__ == "__main__":
    sys.exit(main())